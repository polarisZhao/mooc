{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  para_regul = tf.placeholder(tf.float32)\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + para_regul * tf.nn.l2_loss(weights)\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 19.759792\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 11.4%\n",
      "Minibatch loss at step 500: 2.639395\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 1000: 1.855252\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 1500: 0.957508\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 79.7%\n",
      "Minibatch loss at step 2000: 0.815693\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 2500: 0.833164\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 3000: 0.775074\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 82.0%\n",
      "Test accuracy: 88.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, para_regul: 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_nums = 1024   #[Hidden layers nums]\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  para_regul = tf.placeholder(tf.float32)  \n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, hidden_nums]))\n",
    "  biases = tf.Variable(tf.zeros([hidden_nums]))\n",
    "    \n",
    "  # Hidden computation\n",
    "  hidden_input = tf.nn.relu(tf.matmul(tf_train_dataset, weights) + biases)\n",
    "    \n",
    "  # Hidden Variables  \n",
    "  hidden_weight = tf.Variable(\n",
    "    tf.truncated_normal([hidden_nums, num_labels]))\n",
    "  hidden_biases = tf.Variable(tf.zeros([num_labels]))  \n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(hidden_input, hidden_weight) + hidden_biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + para_regul * (tf.nn.l2_loss(weights) + tf.nn.l2_loss(hidden_weight))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights) + biases), hidden_weight) + hidden_biases)   \n",
    "  test_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights) + biases), hidden_weight) + hidden_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "acc_val = []\n",
    "num_steps = 3001\n",
    "\n",
    "for regul in regul_val: \n",
    "    with tf.Session(graph=graph) as session:\n",
    "      tf.initialize_all_variables().run()\n",
    "      for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, para_regul : regul}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "      acc_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfEAAAFiCAYAAADmwOGQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f7b4350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(regul_val, acc_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_nums = 1024   #[Hidden layers nums]\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  para_regul = tf.placeholder(tf.float32)  \n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, hidden_nums]))\n",
    "  biases = tf.Variable(tf.zeros([hidden_nums]))\n",
    "    \n",
    "  # Hidden computation\n",
    "  hidden_input = tf.nn.relu(tf.matmul(tf_train_dataset, weights) + biases)\n",
    "    \n",
    "  # Hidden Variables  \n",
    "  hidden_weight = tf.Variable(\n",
    "    tf.truncated_normal([hidden_nums, num_labels]))\n",
    "  hidden_biases = tf.Variable(tf.zeros([num_labels]))  \n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(hidden_input, hidden_weight) + hidden_biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights) + biases), hidden_weight) + hidden_biases)   \n",
    "  test_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights) + biases), hidden_weight) + hidden_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 406.192017\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 31.2%\n",
      "Minibatch loss at step 50: 19.237749\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 73.4%\n",
      "Minibatch loss at step 100: 0.000011\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 150: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 200: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 250: 0.000002\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 300: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 350: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 400: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 450: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.6%\n",
      "Test accuracy: 84.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 501\n",
    "train_size = 1024\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_size - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    \n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_nums = 1024   #[Hidden layers nums]\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  para_regul = tf.placeholder(tf.float32)  \n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, hidden_nums]))\n",
    "  biases = tf.Variable(tf.zeros([hidden_nums]))\n",
    "    \n",
    "  # Hidden computation\n",
    "  hidden_input = tf.nn.dropout(tf.nn.relu(tf.matmul(tf_train_dataset, weights) + biases),0.5)\n",
    "    \n",
    "  # Hidden Variables  \n",
    "  hidden_weight = tf.Variable(\n",
    "    tf.truncated_normal([hidden_nums, num_labels]))\n",
    "  hidden_biases = tf.Variable(tf.zeros([num_labels]))  \n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(hidden_input, hidden_weight) + hidden_biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights) + biases), hidden_weight) + hidden_biases)   \n",
    "  test_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights) + biases), hidden_weight) + hidden_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 432.381958\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 30.2%\n",
      "Minibatch loss at step 50: 3.112052\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 77.4%\n",
      "Minibatch loss at step 100: 12.195364\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 77.7%\n",
      "Minibatch loss at step 150: 5.106567\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 79.0%\n",
      "Minibatch loss at step 200: 2.473001\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 79.3%\n",
      "Minibatch loss at step 250: 5.107832\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 300: 0.000126\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 350: 0.132940\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 400: 1.507009\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 450: 0.723428\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.6%\n",
      "Test accuracy: 87.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 501\n",
    "train_size = 1024\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_size - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_nums = 1024   #[Hidden layers nums]\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  para_regul = tf.placeholder(tf.float32)  \n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  global_step = tf.Variable(0)  \n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.65, staircase=True)  \n",
    "    \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, hidden_nums]))\n",
    "  biases = tf.Variable(tf.zeros([hidden_nums]))\n",
    "    \n",
    "  # Hidden computation\n",
    "  hidden_input = tf.nn.relu(tf.matmul(tf_train_dataset, weights) + biases)\n",
    "    \n",
    "  # Hidden Variables  \n",
    "  hidden_weight = tf.Variable(\n",
    "    tf.truncated_normal([hidden_nums, num_labels]))\n",
    "  hidden_biases = tf.Variable(tf.zeros([num_labels]))  \n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(hidden_input, hidden_weight) + hidden_biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + para_regul * (tf.nn.l2_loss(weights) + tf.nn.l2_loss(hidden_weight))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights) + biases), hidden_weight) + hidden_biases)   \n",
    "  test_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights) + biases), hidden_weight) + hidden_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 650.437256\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 28.6%\n",
      "Minibatch loss at step 500: 196.340942\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 1000: 116.125763\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 1500: 81.834709\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at step 2000: 58.695717\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at step 2500: 47.505424\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 3000: 38.575928\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 3500: 33.764740\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 4000: 29.296043\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 4500: 26.886030\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 5000: 24.608055\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 5500: 23.207024\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 6000: 21.990831\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 6500: 21.127554\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 7000: 20.431849\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 7500: 19.972675\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 8000: 19.578583\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 8500: 19.086779\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 9000: 18.884266\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 9500: 18.680748\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 10000: 18.439745\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 10500: 18.320093\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 11000: 18.144794\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 11500: 18.149481\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 12000: 18.202557\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 87.2%\n",
      "Test accuracy: 93.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 12001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, para_regul: 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multi-layer neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden1_nums = 1024\n",
    "hidden2_nums = 256\n",
    "hidden3_nums = 128\n",
    "keep_rate = 0.5\n",
    "\n",
    "def weight_var(layer_num1,layer_num2):\n",
    "    return tf.Variable(tf.truncated_normal([layer_num1, layer_num2], stddev=np.sqrt(2.0 / layer_num1)))\n",
    "\n",
    "def bias_var(layer_nums):\n",
    "    return tf.Variable(tf.zeros([layer_nums]))\n",
    "\n",
    "def compute_logits(input_data, weightss, biasess, dropout_vals=None):  \n",
    "    temp = input_data  \n",
    "    if dropout_vals:  \n",
    "        for w,b,d in zip(weightss[:-1], biasess[:-1], dropout_vals[:-1]):  \n",
    "            temp = tf.nn.relu_layer(tf.nn.dropout(temp, d), w, b)  \n",
    "        temp = tf.matmul(temp, weightss[-1]) + biasess[-1]  \n",
    "    else:  \n",
    "        for w,b in zip(weightss[:-1], biasess[:-1]): \n",
    "            temp = tf.nn.relu_layer(temp, w, b)  \n",
    "        temp = tf.matmul(temp, weightss[-1]) + biasess[-1]  \n",
    "    return temp \n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = weight_var(image_size * image_size, hidden1_nums)  \n",
    "  biases1 = bias_var(hidden1_nums)\n",
    "  weights2 = weight_var(hidden1_nums, hidden2_nums)\n",
    "  biases2 = bias_var(hidden2_nums)\n",
    "  weights3 = weight_var(hidden2_nums, hidden3_nums)\n",
    "  biases3 = bias_var(hidden3_nums)\n",
    "  weights4 = weight_var(hidden3_nums, num_labels)\n",
    "  biases4 = bias_var(num_labels)\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = compute_logits(tf_train_dataset, [weights1, weights2, weights3,weights4], [biases1,biases2,biases3,biases4],   \n",
    "                            dropout_vals=(1.0,0.95,0.95,0.95,1.0))  \n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 4000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(compute_logits(tf_valid_dataset, [weights1, weights2, weights3,weights4], [biases1,biases2,biases3,biases4]))\n",
    "  test_prediction = tf.nn.softmax(compute_logits(tf_test_dataset, [weights1, weights2, weights3,weights4], [biases1,biases2,biases3,biases4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.358826\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 19.5%\n",
      "Minibatch loss at step 2000: 0.253241\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 4000: 0.281373\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 6000: 0.396009\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 8000: 0.406676\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 10000: 0.188852\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 12000: 0.204013\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 14000: 0.240203\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 16000: 0.090496\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 18000: 0.058926\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 20000: 0.179576\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.3%\n",
      "Test accuracy: 96.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 2000 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
